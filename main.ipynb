{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV39pA88SHLerjyrsNZKER",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbron64/CS550_Lab4/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xXJB90VhI1kR"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(X, W):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a linear transformation.\n",
        "\n",
        "    Consider a linear layer that accepts inputs with D features,\n",
        "    and has M neurons.  Assume that our minibatch size\n",
        "    is N.  In other words, we wish to process N samples at once.\n",
        "\n",
        "    The input X has shape (N, D) and contains a minibatch of N\n",
        "    samples, where each sample X[i] has shape (D).  Each sample\n",
        "    will be transformed to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array containing input data, of shape (N, D)\n",
        "    - W: A numpy array of weights, of shape (D+1, M)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (X, W)\n",
        "\n",
        "    The returned (X, W) is redundant, but makes the training code\n",
        "    more concise.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    out = None # Initialize the out variable.\n",
        "\n",
        "    #\n",
        "    # PUT YOUR CODE BELOW: Below, implement the linear forward pass. Store the result in out.\n",
        "    # Make sure to do the bias trick!\n",
        "    #\n",
        "\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "\n",
        "    out = None\n",
        "\n",
        "    #column of ones to X for the bias trick\n",
        "    X_augmented = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
        "\n",
        "    #forward pass\n",
        "    out = np.dot(X_augmented, W)\n",
        "\n",
        "    #cache for backward pass\n",
        "    cache = (X, W)\n",
        "\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "fFgOTPoxFfCb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(d_upstream, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an linear layer.\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - X: Input data, of shape (N, D)\n",
        "      - W: Weights, of shape (D+1, M)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dX: Gradient of the output of this layer with respect to X, of shape (N, D).\n",
        "          This is the downstream gradient.\n",
        "    - dW: Gradient with respect to W, of shape (D+1, M)\n",
        "    \"\"\"\n",
        "    X, W = cache\n",
        "    dX, dW = None, None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the linear backward pass by calculating the\n",
        "    # gradient with respect to the cached inputs X and W. Store them in the\n",
        "    # variables dX and dW.\n",
        "\n",
        "\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "    # For the backward pass, we need to consider that we used the bias trick\n",
        "    # in the forward pass by augmenting X with a column of ones\n",
        "\n",
        "    #compute dX without the bias column\n",
        "    dX = np.dot(d_upstream, W[:-1, :].T)\n",
        "\n",
        "    # Augmented X with ones column for bias\n",
        "    X_augmented = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
        "\n",
        "    #Compute dW\n",
        "    dW = np.dot(X_augmented.T, d_upstream)\n",
        "\n",
        "    return dX, dW"
      ],
      "metadata": {
        "id": "9hnoRGFuFtoC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finite_difference_linear(d_upstream, cache, h):\n",
        "    '''\n",
        "    Computes the numerical gradient for a linear layer\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - X: Input data, of shape (N, D)\n",
        "      - W: Weights, of shape (D+1, M)\n",
        "    - h: The h to use in the finite difference.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dX: Gradient with respect to X, of shape (N, D).  This is the downstream\n",
        "          gradient.\n",
        "    - dW: Gradient with respect to W, of shape (D+1, M)\n",
        "    '''\n",
        "\n",
        "    dX = None\n",
        "    dW = None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the finite difference for the linear\n",
        "    # function.  Return the gradient at input (X,W) w.r.t to x and w.\n",
        "    X, W = cache\n",
        "    dX = np.zeros_like(X)\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #compute output Y given X and W\n",
        "    def compute_output(X, W):\n",
        "        X_augmented = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
        "        return np.dot(X_augmented, W)\n",
        "\n",
        "    #compute dX using finite difference\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            #copy of positive and negative perturbations\n",
        "            X_plus = X.copy()\n",
        "            X_minus = X.copy()\n",
        "\n",
        "            #apply perturbations\n",
        "            X_plus[i, j] += h\n",
        "            X_minus[i, j] -= h\n",
        "\n",
        "            #compute outputs with perturbations\n",
        "            Y_plus = compute_output(X_plus, W)\n",
        "            Y_minus = compute_output(X_minus, W)\n",
        "\n",
        "            #compute finite difference and multiply by gradients\n",
        "            local_gradient = (Y_plus - Y_minus) / (2 * h)\n",
        "            dX[i, j] = np.sum(local_gradient * d_upstream)\n",
        "\n",
        "    #compute dW using finite difference\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            #copies of positive and negative perturbations\n",
        "            W_plus = W.copy()\n",
        "            W_minus = W.copy()\n",
        "\n",
        "            #apply perturbations\n",
        "            W_plus[i, j] += h\n",
        "            W_minus[i, j] -= h\n",
        "\n",
        "            #compute outputs with perturbations\n",
        "            Y_plus = compute_output(X, W_plus)\n",
        "            Y_minus = compute_output(X, W_minus)\n",
        "\n",
        "            #compute finite difference and multiply by gradients\n",
        "            local_gradient = (Y_plus - Y_minus) / (2 * h)\n",
        "            dW[i, j] = np.sum(local_gradient * d_upstream)\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    return dX, dW"
      ],
      "metadata": {
        "id": "egYo2lZgGCNn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_check_linear():\n",
        "    N = 16\n",
        "    D = 4\n",
        "    C = 3\n",
        "\n",
        "    test_weight = np.random.random((D+1, C))\n",
        "    test_input = np.random.random((N, D))\n",
        "    dout = np.random.random((N, C))\n",
        "\n",
        "    cache = (test_input, test_weight)\n",
        "\n",
        "    grad_x_numerical, grad_w_numerical = finite_difference_linear(dout, cache, 1E-9)\n",
        "    grad_x_analytical, grad_w_analytical = linear_backward(dout, cache)\n",
        "\n",
        "    check_input_gradient = np.allclose(grad_x_numerical, grad_x_analytical)\n",
        "    check_weight_gradient = np.allclose(grad_w_numerical, grad_w_analytical)\n",
        "\n",
        "    if not check_input_gradient:\n",
        "        print(\"The gradient with respect to x failed\")\n",
        "\n",
        "    if not check_weight_gradient:\n",
        "        print(\"The gradient respect to w failed\")\n",
        "    print()\n",
        "    print(\"gradient check for linear passed!\")\n",
        "\n",
        "gradient_check_linear()"
      ],
      "metadata": {
        "id": "ZGOkVJjfGT1P",
        "outputId": "f6e82cfb-47a2-4568-81c2-9f6dfaa33d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gradient check for linear passed!\n"
          ]
        }
      ]
    }
  ]
}